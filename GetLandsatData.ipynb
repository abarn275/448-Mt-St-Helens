{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in a301_lib init\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from pystac_client import Client\n",
    "from shapely.geometry import Point\n",
    "from rasterio.windows import Window\n",
    "from pyproj import CRS\n",
    "import rioxarray\n",
    "from xarray import Dataset\n",
    "\n",
    "import a301_lib\n",
    "import pystac\n",
    "import pandas as pd\n",
    "import xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to find sentinel data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landsat_dataset(date, lon, lat, window, bands=None):\n",
    "    \"\"\"\n",
    "    retrieve windowed bands specified in the bands variable.\n",
    "    Save the clipped geotiffs as xarray.DattArrays, returned\n",
    "    in an xarray Dataset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    date: str\n",
    "       date in the form yyy-mm-yy\n",
    "    lon: float\n",
    "       longitude of point in the scene (degrees E)\n",
    "    lat: \n",
    "        latitude of point in the scene (degrees N)\n",
    "    window: rasterio.Window\n",
    "        window for clipping the scene to a subscene\n",
    "    bands: list\n",
    "        list of bands in the form ['B01','B02',...]\n",
    "        the default is ['B04','B05','B06']\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    the_dataset: xarray.Dataset\n",
    "       dataset with rioxarrays of requested bands plus Fmask\n",
    "    \"\"\"\n",
    "    if bands is None:\n",
    "        bands = ['B04','B05','B06']\n",
    "    #\n",
    "    # set up the search -- we are looking for only 1 scene per date\n",
    "    #\n",
    "    the_point = Point(lon, lat)\n",
    "    cmr_api_url = \"https://cmr.earthdata.nasa.gov/stac/LPCLOUD\"\n",
    "    client = Client.open(cmr_api_url)\n",
    "    \n",
    "    search = client.search(\n",
    "        collections=[\"HLSL30.v2.0\"],\n",
    "        intersects=the_point,\n",
    "        datetime= date\n",
    "    )\n",
    "    items = search.get_all_items()\n",
    "    print(f\"found {len(items)} item\")\n",
    "    #\n",
    "    # get the metadata and add date, cloud_cover and band_name to the new DataArrays\n",
    "    #\n",
    "    props = items[0].properties\n",
    "    out_dict = {}\n",
    "    bands.extend(['Fmask'])\n",
    "    for the_band in bands:\n",
    "        print(f\"inside get_landsat_scene: reading {the_band}\")\n",
    "        href = items[0].assets[the_band].href\n",
    "        print(href)\n",
    "        lazy_ds = rioxarray.open_rasterio(href,mask_and_scale=True)\n",
    "        #\n",
    "        # now read the window\n",
    "        #\n",
    "        clipped_ds = lazy_ds.rio.isel_window(window)\n",
    "        #\n",
    "        # add some custom attributes\n",
    "        #\n",
    "        clipped_ds.attrs['date'] = props['datetime'] #date and time\n",
    "        clipped_ds.attrs['cloud_cover'] = props['eo:cloud_cover']\n",
    "        clipped_ds.attrs['band_name'] = the_band\n",
    "        print(clipped_ds.attrs)\n",
    "        utm_zone = clipped_ds.attrs['HORIZONTAL_CS_NAME'][-3:-1].strip()\n",
    "        if lat < 0:\n",
    "            is_southern=True\n",
    "        else:\n",
    "            is_southern=False\n",
    "        clipped_ds.attrs['cartopy_epsg_code'] = find_epsg_code(utm_zone,south=is_southern)\n",
    "        clipped_ds.attrs['day']=props['datetime'][:10]  #yyyy-mm-dd\n",
    "        out_dict[the_band] = clipped_ds\n",
    "    #\n",
    "    # convert the mask to 1=no cloud over land, np.nan=otherwise\n",
    "    #\n",
    "    out_dict['Fmask'] = get_clear_mask(out_dict['Fmask'])\n",
    "    coords = out_dict['Fmask'].coords\n",
    "    attrs = out_dict['Fmask'].attrs\n",
    "    dataset = Dataset(data_vars = out_dict, coords = coords, attrs = attrs )\n",
    "    return dataset\n",
    "\n",
    "def find_epsg_code(utm_zone, south=False):\n",
    "    \"\"\"\n",
    "    https://gis.stackexchange.com/questions/365584/convert-utm-zone-into-epsg-code\n",
    "    \n",
    "    cartopy wants crs names as epsg codes, i.e. UTM zone 10N is EPSG:32610, 10S is EPSG:32710\n",
    "    \"\"\"\n",
    "    crs = CRS.from_dict({'proj': 'utm', 'zone': utm_zone, 'south': south})\n",
    "    epsg, code = crs.to_authority()\n",
    "    cartopy_epsg_code = code\n",
    "    return cartopy_epsg_code\n",
    "\n",
    "def get_clear_mask(fmask_ds):\n",
    "    \"\"\"\n",
    "    return a DataArray copy of fmask_ds but with the pixel values set to\n",
    "    1 where there is both no cloud and pixel is land, and np.nan where there is cloud\n",
    "    or the pixel is water\n",
    "    \n",
    "    The bit patterns from the HSL QA mask are:\n",
    "     \n",
    "    Bits are listed from the MSB (bit 7) to the LSB (bit 0): \n",
    "    7-6    aerosol:\n",
    "           00 - climatology\n",
    "           01 - low\n",
    "           10 - average\n",
    "           11 - high\n",
    "    5      water\n",
    "    4      snow/ice\n",
    "    3      cloud shadow\n",
    "    2      adjacent to cloud\n",
    "    1      cloud\n",
    "    0      cirrus cloud\n",
    "    \n",
    "    so a bit mask of 0b00100011 when anded with a QA value\n",
    "    will return non-zero when there is water, a cloud, or a cirrus cloud\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    fmask_ds: xarray DataArray\n",
    "       landsat or sentinel hls scene create with rioxarray.open_rasterio\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    clearmask_ds: xarray DataArray\n",
    "      new array with the clear sky mask for each pixel set to 1 if clear, np.nan if water or cloud\n",
    "    \"\"\"\n",
    "    #\n",
    "    # convert float32 to unsigned 8 bit integer\n",
    "    #\n",
    "    bit_mask = fmask_ds.data.astype(np.uint8)\n",
    "    ref_mask = np.zeros_like(bit_mask)\n",
    "    #\n",
    "    # don't destroy original fmask DataArray\n",
    "    #\n",
    "    clearmask_ds = copy.deepcopy(fmask_ds)\n",
    "    #\n",
    "    # work with unsigned 8 bit values instead of\n",
    "    # base 10 floats\n",
    "    #\n",
    "    bit_mask = fmask_ds.data.astype(np.uint8)\n",
    "    #\n",
    "    # create a reference mask that will select\n",
    "    # bits 5, 1 and 0, which we want tocheck\n",
    "    #\n",
    "    ref_mask = np.zeros_like(bit_mask)\n",
    "    ref_mask[...] = 0b00100011  #find water (bit 5), cloud (bit 1) , cirrus (bit 0)\n",
    "    #\n",
    "    # if all three of those bits are 0, then bitwise_and will return 0\n",
    "    # otherwise it will return either a value greater than 0\n",
    "    #\n",
    "    cloudy_values = np.bitwise_and(bit_mask,ref_mask)\n",
    "    cloudy_values[cloudy_values>0]=1  #cloud or water\n",
    "    cloudy_values[cloudy_values==0]=0 #rest of scene\n",
    "    #\n",
    "    # now invert this, writing np.nan where there is \n",
    "    # cloud or water.  Go back to float32 so we can use np.nan\n",
    "    #\n",
    "    clear_mask = cloudy_values.astype(np.float32)\n",
    "    clear_mask[cloudy_values == 1]=np.nan\n",
    "    clear_mask[cloudy_values == 0]=1\n",
    "    clearmask_ds.data = clear_mask\n",
    "    return clearmask_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_lon, ana_lat = -122.223611, 46.257066 #St Helens analysis region\n",
    "con_lat, con_lon = 46.190630, -122.286143 #St Helens control region\n",
    "ana_location = Point(ana_lon, ana_lat)\n",
    "con_location = Point(con_lon, con_lat)\n",
    "date_range = \"2013-01-01/2024-12-31\"\n",
    "#\n",
    "# filename to save the dataframe for future analysis\n",
    "#\n",
    "csv_filename = a301_lib.data_share / \"ajb/landsat/StHelens_search.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282\n"
     ]
    }
   ],
   "source": [
    "# connect to the STAC endpoint\n",
    "cmr_api_url = \"https://cmr.earthdata.nasa.gov/stac/LPCLOUD\"\n",
    "client = Client.open(cmr_api_url)\n",
    "\n",
    "search = client.search(\n",
    "    collections=[\"HLSL30.v2.0\"],\n",
    "    intersects=ana_location,\n",
    "    datetime= date_range\n",
    ") \n",
    "\n",
    "items = search.get_all_items()\n",
    "print(len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scene': 0,\n",
       " 'cloud_cover': 86,\n",
       " 'datetime': datetime.datetime(2013, 4, 14, 18, 57, 28, 350000, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_list = []\n",
    "for index, value in enumerate(items):\n",
    "    props = value.properties\n",
    "    the_date = pystac.utils.str_to_datetime(props['datetime'])\n",
    "    scene_dict = dict(scene = index,\n",
    "                      cloud_cover = props['eo:cloud_cover'],\n",
    "                      datetime = the_date \n",
    "                       )\n",
    "    scene_list.append(scene_dict)\n",
    "\n",
    "scene_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_df = pd.DataFrame.from_records(scene_list)\n",
    "\n",
    "def make_seasoncol(row):\n",
    "    seasons = {'djf':[12,1,2],\n",
    "               'mam':[3,4,5],\n",
    "               'jja':[6,7,8],\n",
    "               'son':[9,10,11]}\n",
    "    for season,months in seasons.items():\n",
    "        month = row['datetime'].month\n",
    "        year = row['datetime'].year\n",
    "        if month in months:\n",
    "            #\n",
    "            # the winter of 2013 begins in\n",
    "            # december 2012.  So the year of the\n",
    "            # scene and the year of the season diverge\n",
    "            #\n",
    "            if month == 12:\n",
    "                row['season_year'] = year + 1\n",
    "            else:\n",
    "                row['season_year'] = year\n",
    "            row['season']=season\n",
    "            row['year']= year\n",
    "            row['month']= month\n",
    "            row['day']= row['datetime'].day\n",
    "    return row\n",
    "\n",
    "new_df = the_df.apply(make_seasoncol,axis=1)\n",
    "new_df = new_df[['scene','cloud_cover','season','year','season_year','month','day']]\n",
    "new_df.head()\n",
    "\n",
    "#csv_filename = a301_lib.data_share / \"ajb/sentinel/StHelens_search.csv\"\n",
    "new_df.to_csv(csv_filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clear_df = the_df[the_df['cloud_cover'] < 50]\n",
    "len(clear_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_df = new_df.groupby(['season_year','season'])\n",
    "season_dict = dict(list(season_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scene            65\n",
       "cloud_cover      15\n",
       "season          jja\n",
       "year           2016\n",
       "season_year    2016\n",
       "month             7\n",
       "day              27\n",
       "Name: 65, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "season_dict[(2016,'jja')]['cloud_cover']\n",
    "season_dict[(2016,'jja')].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "def find_min(a_df):\n",
    "    \"\"\"\n",
    "    What does this function do?\n",
    "    \"\"\"\n",
    "    min_row = a_df['cloud_cover'].argmin()\n",
    "    return min_row\n",
    "\n",
    "#\n",
    "# explain this loop\n",
    "#\n",
    "out_list = []\n",
    "for the_key, a_df in season_dict.items():\n",
    "    min_row = find_min(a_df)\n",
    "    min_scene = a_df.iloc[min_row]\n",
    "    the_series = pd.Series(min_scene)\n",
    "    out_list.append(the_series)\n",
    "    \n",
    "new_frame = pd.DataFrame.from_records(out_list, index='scene')\n",
    "\n",
    "new_frame.head()\n",
    "print(len(new_frame))\n",
    "\n",
    "csv_filename = a301_lib.data_share / \"ajb/landsat/ClearStHelens_search.csv\"\n",
    "new_frame.to_csv(csv_filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotiff_dir = a301_lib.sat_data / \"ajb/landsat/ndvi_geotiffs\"\n",
    "geotiff_dir.mkdir(exist_ok = True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GDAL_HTTP_COOKIEFILE\"] = \"./cookies.txt\"\n",
    "os.environ[\"GDAL_HTTP_COOKIEJAR\"] = \"./cookies.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write analysis images\n",
    "do_write=False\n",
    "if do_write:\n",
    "    the_window = Window(col_off=1928, row_off=2503, width=134, height=135)\n",
    "    for row_num in np.arange(0,48):\n",
    "        row = new_frame.iloc[row_num]\n",
    "        year,month,day = row['year'],row['month'],row['day']\n",
    "        the_date = f\"{year:02d}-{month:02d}-{day:02d}\"\n",
    "        print(the_date)\n",
    "        the_scene = get_landsat_dataset(the_date, ana_lon, ana_lat, the_window, bands = ['B04',\"B05\"]) \n",
    "        file_path = geotiff_dir / f\"analysis/testlandsat_{the_date}_StHelens.nc\"\n",
    "        print(f\"saving to {file_path}\")\n",
    "        the_scene.to_netcdf(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write control images\n",
    "do_write=False\n",
    "if do_write:\n",
    "    the_window = Window(col_off=1770, row_off=2751, width=134, height=134)\n",
    "    for row_num in np.arange(0,48):\n",
    "        row = new_frame.iloc[row_num]\n",
    "        year,month,day = row['year'],row['month'],row['day']\n",
    "        the_date = f\"{year:02d}-{month:02d}-{day:02d}\"\n",
    "        print(the_date)\n",
    "        the_scene = get_landsat_dataset(the_date, con_lon, con_lat, the_window, bands = ['B04',\"B05\"]) \n",
    "        file_path = geotiff_dir / f\"control/landsat_{the_date}_StHelens.nc\"\n",
    "        print(f\"saving to {file_path}\")\n",
    "        the_scene.to_netcdf(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ndvi(the_ds):\n",
    "    #\n",
    "    # xarray was unhappy with the extra third dimension\n",
    "    # for the landsat bands:  [1, nrows, ncols]\n",
    "    # so squeeze it out\n",
    "    #\n",
    "    the_ds = the_ds.squeeze()\n",
    "    fmask = the_ds['Fmask']\n",
    "    NIR = the_ds['B05']*fmask.data\n",
    "    RED = the_ds['B04']*fmask.data\n",
    "    ndvi  = (NIR - RED)/(NIR + RED)\n",
    "    #\n",
    "    # Fmask doesn't find every bad pixel, so go ahead\n",
    "    # and set pixels to np.nan for any ndvi not between 0-1\n",
    "    #\n",
    "    ndvi.data[ndvi.data < 0] = np.nan\n",
    "    ndvi.data[ndvi.data > 1] = np.nan\n",
    "    #\n",
    "    # Make a new dataArray \n",
    "    #\n",
    "    ndvi_array = xarray.DataArray(data = ndvi, dims = [\"y\",\"x\"])\n",
    "    #\n",
    "    # you'll get nan conversion errors unless you specifiy nan as\n",
    "    # your missing value\n",
    "    #\n",
    "    ndvi_array.rio.write_nodata(np.nan, inplace=True)\n",
    "    #\n",
    "    # copy the crs and affine transform from band 4\n",
    "    #\n",
    "    ndvi_array.rio.write_crs(RED.rio.crs, inplace=True)\n",
    "    ndvi_array.rio.write_transform(RED.rio.transform(), inplace=True)\n",
    "    #\n",
    "    # add some attributes\n",
    "    #\n",
    "    ndvi_array = ndvi_array.assign_attrs({'day':the_ds.day,\n",
    "                                          'band_name':'ndvi',\n",
    "                                          'history':'written by write_ndvi notebook'})\n",
    "    #\n",
    "    # add the ndvi_array to the dataset and return\n",
    "    #\n",
    "    ndvi_dataset = the_ds.assign(variables = {'ndvi' : ndvi_array})\n",
    "    return ndvi_dataset\n",
    "\n",
    "def write_ndvi_array(in_dir, out_dir, write_it):\n",
    "    '''\n",
    "    Writes a new array to the geotiff netCDF files in \"in_dir\" and writes them to \"out_dir\"\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    in_dir: PosixPath\n",
    "        PosixPath to directory with the original netCDF for analysis\n",
    "    \n",
    "    out_dir: PosixPath\n",
    "        PosixPath to the directoy to store the new netCDF files with NDVI array\n",
    "    \n",
    "    write_it: Boolean\n",
    "        True to write files new files, False to not\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    string message of result\n",
    "    '''\n",
    "    if write_it:\n",
    "        in_files = list(in_dir.glob(\"*nc\"))\n",
    "        for the_file in in_files:\n",
    "            the_ds = rioxarray.open_rasterio(the_file,mode = 'r',mask_and_scale = True)\n",
    "            #\n",
    "            # Give the file the same name, but put it in the new folder\n",
    "            #\n",
    "            out_file = out_dir / the_file.name\n",
    "            new_ds = calc_ndvi(the_ds)\n",
    "            new_ds.to_netcdf(out_file)\n",
    "        return \"dataset downloaded\"\n",
    "    else:\n",
    "        return \"dataset not downloaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset not downloaded'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_dir = a301_lib.sat_data / \"ajb/landsat/ndvi_geotiffs/analysis\"\n",
    "out_dir = a301_lib.sat_data / \"ajb/landsat/ndvi_geotiffs_output/analysis\"\n",
    "in_dir.mkdir(exist_ok = True, parents=True)\n",
    "out_dir.mkdir(exist_ok = True, parents=True)\n",
    "\n",
    "write_ndvi_array(in_dir, out_dir, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset not downloaded'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_dir = a301_lib.sat_data / \"ajb/landsat/ndvi_geotiffs/control\"\n",
    "out_dir = a301_lib.sat_data / \"ajb/landsat/ndvi_geotiffs_output/control\"\n",
    "in_dir.mkdir(exist_ok = True, parents=True)\n",
    "out_dir.mkdir(exist_ok = True, parents=True)\n",
    "\n",
    "write_ndvi_array(in_dir, out_dir, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n",
    "catalog = Client.open(f'{STAC_URL}/USGS_EROS/')\n",
    "products = [c for c in catalog.get_children()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISERV_1: International Space Station SERVIR Environmental Research and Visualization System V1\n",
      "Landsat Level-1 Collection 2_Collection 2: Landsat Level-1 Collection 2\n",
      "Landsat Level-2 Surface Reflectance Collection 2_Collection 2: Landsat Level-2 Surface Reflectance Collection 2\n",
      "Landsat Level-2 Surface Temperature Collection 2_Collection 2: Landsat Level-2 Surface Temperature Collection 2\n"
     ]
    }
   ],
   "source": [
    "for p in products: \n",
    "    print(f\"{p.id}: {p.title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a301",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
